{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93b3f6f7-0eca-4723-bfee-b6f69594560b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from utils import MLP, Trainer, set_seed, model_builder\n",
    "from torchvision import datasets\n",
    "import torch.nn as nn\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "\n",
    "seed = 42 # for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1a81be-e805-41a7-b68a-52d6139d35a6",
   "metadata": {},
   "source": [
    "## Regularization in Neural Networks\n",
    "\n",
    "In this tutorial, we explore **regularization techniques** in the context of training a neural network using the MNIST dataset. Regularization is essential for controlling overfitting and improving generalization by applying constraints to the model. We will use **L2 regularization (weight decay)**, but you can also switch to **L1 regularization**. Additionally, we’ll explore **dropout** and **early stopping** as forms of regularization.\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "We begin by setting important hyperparameters for the training:\n",
    "\n",
    "- **Number of epochs** (`num_epochs`): Defines the total number of times the entire dataset is passed through the network during training. We've set this to 10, allowing the model to train long enough while preventing excessive overfitting.\n",
    "- **Batch size** (`batch_size`): This specifies how many samples to process at once before updating the model’s weights. A batch size of 64 is commonly used for balanced performance.\n",
    "- **K-fold cross-validation** (`k_folds`): We use 3 folds, which means the dataset will be split into 3 parts, training the model on two of them and validating on the third, ensuring better generalization by training and validating multiple times.\n",
    "- **Hidden neurons** (`hidden_neurons`): The architecture of the model has 3 layers with 32, 32, and 16 neurons, respectively.\n",
    "- **Regularization type** (`reg_type`): We specify L2 regularization, which penalizes large weights, thus forcing the model to prefer smaller weights, helping avoid overfitting.\n",
    "- **Regularization strength** (`reg_lambda`): A value of 0.01 is used for the strength of regularization, which controls how much penalty is applied. You can increase or decrease this depending on the level of regularization you need.\n",
    "- **Early stopping** (`early_stopping`): Early stopping is disabled for now, but it can be enabled to stop training if the validation performance stops improving after a few epochs, which is determined by the `patience` parameter. Early stopping itself acts as a form of regularization because it prevents the model from training too long and overfitting.\n",
    "- **Dropout rate** (`dropout_rate`): Dropout is a regularization technique that randomly \"drops\" a percentage of neurons during training, preventing the network from over-relying on specific neurons. Here, we set it to 0, meaning no dropout is applied.\n",
    "\n",
    "## Dataset and Transformation\n",
    "\n",
    "The dataset used is the **MNIST** dataset, which consists of grayscale images of handwritten digits (0-9). We apply two common transformations:\n",
    "\n",
    "1. **ToTensor**: Converts the images to PyTorch tensors and scales the pixel values to the range [0, 1].\n",
    "2. **Normalization**: We normalize the images using the precomputed mean (`0.1307`) and standard deviation (`0.3081`). This is important for ensuring that the model receives consistent and normalized data, speeding up convergence during training.\n",
    "\n",
    "## Regularization in Cross-Validation\n",
    "\n",
    "We use **K-fold cross-validation** to train and evaluate our model across multiple splits of the data. This ensures that the model is evaluated on different subsets of the data, helping it generalize better.\n",
    "\n",
    "L2 regularization, defined by the **weight decay** term, is applied in the optimizer. This helps prevent the model from becoming too complex by penalizing large weights. If we wanted to use **L1 regularization** (which encourages sparsity by penalizing the absolute value of weights), we could simply change `reg_type` to `'l1'`.\n",
    "\n",
    "## Cross-Validation\n",
    "\n",
    "The `Trainer` class is responsible for managing the training process, including applying regularization, performing cross-validation, and (if enabled) handling early stopping. Early stopping itself acts as a regularization technique because it helps prevent overfitting by halting training once the validation performance starts to deteriorate.\n",
    "\n",
    "Here, the trainer handles everything from creating the model using the `model_builder` function, to applying the correct transformations, and executing the k-fold cross-validation procedure.\n",
    "\n",
    "By the end of the cross-validation, we should have a model that benefits from both L2 regularization (controlling large weights) and the enhanced evaluation provided by cross-validation, making it more robust and less prone to overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e6e69af-05af-41ae-837e-18838ad41ee3",
   "metadata": {},
   "source": [
    "### Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7db6e085-fed2-4f82-8a8f-8bad4e4bb359",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /opt/conda/conda-bld/pytorch_1623448224956/work/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x400 with 6 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data loading and transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(), # transforms the input images to a pytorch tensor\n",
    "                           # in the case of MNIST dataset, the original data is stores in uint8 format\n",
    "                           # so it assumes values ranging from 0 to 255.\n",
    "                           # the ToTensor transform also scales the data to the [0,1] range\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # this removes the pre-caulculated mean (0.1307) and\n",
    "                                               # divides by the standard deviation (0.3081)\n",
    "                                               # to apply a z-score scaling\n",
    "                           # Note: Generaly the correct way of dealing with scaling a dataset is to\n",
    "                           # apply the normalization over each fold by (in the case of z-score scaling)\n",
    "                           # estimating the mean and std deviation from training data and applying it to\n",
    "                           # both training and validation\n",
    "])\n",
    "\n",
    "# Load the MNIST dataset\n",
    "dataset = datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "\n",
    "# Function to plot examples from the dataset\n",
    "def plot_mnist_examples(dataset, num_examples=6):\n",
    "    fig, axes = plt.subplots(1, num_examples, figsize=(10, 4))\n",
    "    \n",
    "    for i in range(num_examples):\n",
    "        # Randomly sample an image from the dataset\n",
    "        img, label = dataset[i]\n",
    "        img = img.squeeze()  # Remove the color channel (1x28x28 -> 28x28)\n",
    "\n",
    "        # Plot the image\n",
    "        axes[i].imshow(img, cmap='gray')\n",
    "        axes[i].set_title(f'Label: {label}')\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot 6 random MNIST examples\n",
    "plot_mnist_examples(dataset, num_examples=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eb6107-c43e-46a4-b121-a6b3736e9c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/3\n",
      "Fold 1, Epoch [1/10], Subtrain Loss: 2.3095, Internal Val Loss: 1.6559\n",
      "Fold 1, Epoch [2/10], Subtrain Loss: 1.1975, Internal Val Loss: 0.9726\n",
      "Fold 1, Epoch [3/10], Subtrain Loss: 0.8912, Internal Val Loss: 0.8385\n",
      "Fold 1, Epoch [4/10], Subtrain Loss: 0.7998, Internal Val Loss: 0.7855\n",
      "Fold 1, Epoch [5/10], Subtrain Loss: 0.7512, Internal Val Loss: 0.7422\n",
      "Fold 1, Epoch [6/10], Subtrain Loss: 0.7186, Internal Val Loss: 0.7148\n",
      "Fold 1, Epoch [7/10], Subtrain Loss: 0.6938, Internal Val Loss: 0.6976\n",
      "Fold 1, Epoch [8/10], Subtrain Loss: 0.6735, Internal Val Loss: 0.6785\n",
      "Fold 1, Epoch [9/10], Subtrain Loss: 0.6582, Internal Val Loss: 0.6655\n",
      "Fold 1, Epoch [10/10], Subtrain Loss: 0.6445, Internal Val Loss: 0.6558\n",
      "Fold 1, Final Validation Loss: 0.6463, Final Validation Accuracy: 92.20%\n",
      "Fold 2/3\n",
      "Fold 2, Epoch [1/10], Subtrain Loss: 2.4522, Internal Val Loss: 2.1475\n",
      "Fold 2, Epoch [2/10], Subtrain Loss: 1.5659, Internal Val Loss: 1.1455\n",
      "Fold 2, Epoch [3/10], Subtrain Loss: 0.9866, Internal Val Loss: 0.8894\n",
      "Fold 2, Epoch [4/10], Subtrain Loss: 0.8378, Internal Val Loss: 0.7970\n",
      "Fold 2, Epoch [5/10], Subtrain Loss: 0.7756, Internal Val Loss: 0.7611\n",
      "Fold 2, Epoch [6/10], Subtrain Loss: 0.7348, Internal Val Loss: 0.7134\n",
      "Fold 2, Epoch [7/10], Subtrain Loss: 0.7032, Internal Val Loss: 0.6887\n",
      "Fold 2, Epoch [8/10], Subtrain Loss: 0.6795, Internal Val Loss: 0.6799\n",
      "Fold 2, Epoch [9/10], Subtrain Loss: 0.6624, Internal Val Loss: 0.6550\n",
      "Fold 2, Epoch [10/10], Subtrain Loss: 0.6470, Internal Val Loss: 0.6432\n",
      "Fold 2, Final Validation Loss: 0.6505, Final Validation Accuracy: 91.70%\n",
      "Fold 3/3\n",
      "Fold 3, Epoch [1/10], Subtrain Loss: 2.2743, Internal Val Loss: 1.6152\n",
      "Fold 3, Epoch [2/10], Subtrain Loss: 1.1412, Internal Val Loss: 0.9158\n",
      "Fold 3, Epoch [3/10], Subtrain Loss: 0.8682, Internal Val Loss: 0.8183\n",
      "Fold 3, Epoch [4/10], Subtrain Loss: 0.7996, Internal Val Loss: 0.7773\n",
      "Fold 3, Epoch [5/10], Subtrain Loss: 0.7547, Internal Val Loss: 0.7369\n",
      "Fold 3, Epoch [6/10], Subtrain Loss: 0.7227, Internal Val Loss: 0.7128\n"
     ]
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 10  # Increased epochs to allow early stopping\n",
    "batch_size = 64\n",
    "k_folds = 3\n",
    "hidden_neurons = (32, 32 ,16)\n",
    "hidden_layers = len(hidden_neurons)\n",
    "reg_type = 'l2'  # Options: 'l1', 'l2', or None\n",
    "reg_lambda = 0.01  # Regularization strength (lambda)\n",
    "early_stopping = False  # Enable early stopping\n",
    "patience = 5  # Number of epochs to wait before stopping\n",
    "dropout_rate = 0.0\n",
    "    \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Set seed for reproducibility\n",
    "set_seed(seed)\n",
    "\n",
    "# Initialize Trainer and start cross-validation with early stopping\n",
    "trainer = Trainer(\n",
    "    model_builder=model_builder(hidden_layers, \n",
    "                                hidden_neurons,\n",
    "                                dropout_rate),\n",
    "    dataset = dataset,\n",
    "    criterion=criterion,\n",
    "    num_epochs=num_epochs,\n",
    "    batch_size=batch_size,\n",
    "    k_folds=k_folds,\n",
    "    reg_type=reg_type,\n",
    "    reg_lambda=reg_lambda,\n",
    "    early_stopping=early_stopping,\n",
    "    patience=patience,\n",
    "    seed=seed  # Set seed for reproducibility\n",
    ")\n",
    "trainer.cross_validate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42486623-3fb6-40f9-ae9f-2377e0d0762d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create grid for the plots\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "gs = GridSpec(2, 2, figure=fig)\n",
    "\n",
    "# Plot the loss (occupying two slots in the upper row)\n",
    "ax_loss = fig.add_subplot(gs[0, :])\n",
    "trainer.plot_loss(ax_loss)\n",
    "\n",
    "# Plot t-SNE in the lower left slot\n",
    "ax_tsne = fig.add_subplot(gs[1, 0])\n",
    "trainer.apply_tsne(ax_tsne)\n",
    "\n",
    "# Plot weights histogram in the lower right slot\n",
    "ax_weights = fig.add_subplot(gs[1, 1])\n",
    "ax_weights.set_yscale(\"log\")\n",
    "trainer.plot_weights_histogram(ax_weights)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c846bc-6139-46ce-b96d-e32b904bd9c2",
   "metadata": {},
   "source": [
    "## Extra: Orthogonal Matching Pursuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacf314e-0fbf-4b65-b02a-5b35c0e96429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import OrthogonalMatchingPursuit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import DictionaryLearning\n",
    "\n",
    "# Step 1: Load MNIST dataset\n",
    "print(\"Loading MNIST data...\")\n",
    "mnist = fetch_openml('mnist_784')\n",
    "X = mnist.data / 255.0  # Normalize to [0, 1]\n",
    "y = mnist.target.astype(int)\n",
    "\n",
    "X = X[::70]\n",
    "y = y[::70]\n",
    "\n",
    "# Use a subset of the MNIST data to speed up the computation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Step 2: Create a Dictionary using DictionaryLearning\n",
    "# We'll use DictionaryLearning to learn a set of dictionary atoms from the MNIST dataset.\n",
    "print(\"Learning the dictionary...\")\n",
    "n_atoms = 100  # Number of dictionary atoms\n",
    "dict_learning = DictionaryLearning(n_components=n_atoms, transform_algorithm='omp', random_state=42, max_iter=100)\n",
    "dictionary = dict_learning.fit(X_train).components_\n",
    "\n",
    "# Step 3: Apply Orthogonal Matching Pursuit (OMP) for sparse coding\n",
    "# We'll apply OMP to sparsely represent test MNIST images using the learned dictionary.\n",
    "omp = OrthogonalMatchingPursuit(n_nonzero_coefs=10)\n",
    "\n",
    "# We will now sparsely code and reconstruct some test images.\n",
    "n_samples_to_reconstruct = 5\n",
    "reconstructed_images = []\n",
    "\n",
    "print(\"Reconstructing test images using OMP...\")\n",
    "for i in range(n_samples_to_reconstruct):\n",
    "    test_image = X_test.iloc[i].to_numpy().reshape(1, -1)  # Flatten the test image\n",
    "\n",
    "    # Fit OMP on the test image using the learned dictionary\n",
    "    omp.fit(dictionary.T, test_image.T)\n",
    "\n",
    "    # Get sparse coefficients\n",
    "    sparse_code = omp.coef_\n",
    "\n",
    "    # Reconstruct the image using the sparse code and dictionary\n",
    "    reconstructed_image = np.dot(sparse_code, dictionary)\n",
    "\n",
    "    # Store the reconstructed image\n",
    "    reconstructed_images.append(reconstructed_image)\n",
    "\n",
    "# Step 4: Plot the original and reconstructed images\n",
    "fig, axes = plt.subplots(2, n_samples_to_reconstruct, figsize=(15, 5))\n",
    "\n",
    "for i in range(n_samples_to_reconstruct):\n",
    "    # Plot original image\n",
    "    axes[0, i].imshow(X_test.iloc[i].to_numpy().reshape(28, 28), cmap='gray')\n",
    "    axes[0, i].set_title(f\"Original Image {i+1}\")\n",
    "    axes[0, i].axis('off')\n",
    "\n",
    "    # Plot reconstructed image\n",
    "    axes[1, i].imshow(reconstructed_images[i].reshape(28, 28), cmap='gray')\n",
    "    axes[1, i].set_title(f\"Reconstructed Image {i+1}\")\n",
    "    axes[1, i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4cb2e6-90c4-4563-b52f-07ae0da17697",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
